{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29878329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nest_asyncio\n",
    "import qdrant_client\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7b1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "595b17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "## qdrant\n",
    "collection_name = \"chat_with_docs\"\n",
    "\n",
    "# docs\n",
    "directory = \"./docs\"\n",
    "\n",
    "EMBED_MODEL = \"BAAI/bge-large-en-v1.5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3818152",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbaf420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=directory, required_exts=[\".pdf\"], recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b5a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab10c4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c431c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='5b2046f5-24b0-4653-8c97-990605dbf2fa', embedding=None, metadata={'page_label': '1', 'file_name': 'dspy.pdf', 'file_path': '/Users/prakharpatidar/Documents/github/rag-sytems/docs/dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2025-05-26', 'last_modified_date': '2025-05-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nDSP Y: C OMPILING DECLARATIVE LANGUAGE\\nMODEL CALLS INTO SELF -IMPROVING PIPELINES\\nOmar Khattab,1 Arnav Singhvi,2\\nParidhi Maheshwari,4 Zhiyuan Zhang,1\\nKeshav Santhanam,1 Sri Vardhamanan,6 Saiful Haq,6\\nAshutosh Sharma,6 Thomas T. Joshi,7 Hanna Moazam,8\\nHeather Miller,3,9 Matei Zaharia,2 Christopher Potts1\\n1Stanford University, 2UC Berkeley, 3Carnegie Mellon University,\\n4Amazon Alexa AI, 5Dashworks Technologies, Inc.,\\n6IIT Bombay, 7Calera Capital, 8Microsoft, 9Two Sigma Investments\\nokhattab@cs.stanford.edu\\nABSTRACT\\nThe ML community is rapidly exploring techniques for prompting language mod-\\nels (LMs) and for stacking them into pipelines that solve complex tasks. Un-\\nfortunately, existing LM pipelines are typically implemented using hard-coded\\n“prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a\\nmore systematic approach for developing and optimizing LM pipelines, we intro-\\nduce DSPy, a programming model that abstracts LM pipelines astext transforma-\\ntion graphs, i.e. imperative computation graphs where LMs are invoked through\\ndeclarative modules. DSPy modules are parameterized, meaning they can learn\\n(by creating and collecting demonstrations) how to apply compositions of prompt-\\ning, finetuning, augmentation, and reasoning techniques. We design a compiler\\nthat will optimize any DSPy pipeline to maximize a given metric. We conduct\\ntwo case studies, showing that succinct DSPy programs can express and optimize\\nsophisticated LM pipelines that reason about math word problems, tackle multi-\\nhop retrieval, answer complex questions, and control agent loops. Within minutes\\nof compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-\\nbootstrap pipelines that outperform standard few-shot prompting (generally by\\nover 25% and 65%, respectively) and pipelines with expert-created demonstra-\\ntions (by up to 5–46% and 16–40%, respectively). On top of that, DSPy pro-\\ngrams compiled to open and relatively small LMs like 770M-parameter T5 and\\nllama2-13b-chat are competitive with approaches that rely on expert-written\\nprompt chains for proprietary GPT-3.5.\\nDSPy is available at https://github.com/stanfordnlp/dspy.\\n1 I NTRODUCTION\\nLanguage models (LMs) are enabling researchers to build NLP systems at higher levels of abstrac-\\ntion and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an\\nexploding space of “prompting” techniques—and lightweight finetuning techniques—for adapting\\nLMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022;\\nWang et al., 2022b), andaugmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al.,\\n2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these tech-\\nniques are explored in isolation, but interest has been growing in building multi-stage pipelines and\\nagents that decompose complex tasks into more manageable calls to LMs in an effort to improve\\nperformance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot\\net al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza & Rafiei, 2023; Shinn et al., 2023).\\nUnfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is\\nexacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM\\n1\\narXiv:2310.03714v1  [cs.CL]  5 Oct 2023', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940a7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a437aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL,\n",
    "                                   trust_remote_code=True)\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "673c4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91beba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11afd50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2309995",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad737192",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a334fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3f25120",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6882a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, DSPy (Declarative Signature-based Prompting for eXplainability) is a programming model that translates prompting techniques into parameterized declarative modules. It allows users to define \"signatures\" - typed declarations of functions that specify what a text transformation needs to do, rather than how a specific LM should be prompted to implement that behavior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a528f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rags",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
